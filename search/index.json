[{"content":"Welcome to Hugo theme Stack. This is your first post. Edit or delete it, then start writing!\nFor more information about this theme, check the documentation: https://stack.jimmycai.com/\nWant a site like this? Check out hugo-theme-stack-stater\nPhoto by Pawel Czerwinski on Unsplash\n","date":"2022-03-06T00:00:00Z","image":"https://zainz004.github.io/p/hello-world/cover_hu6307248181568134095.jpg","permalink":"https://zainz004.github.io/p/hello-world/","title":"Hello World"},{"content":"Intro RDK X5 is a high-performance robotics development kit designed for intelligent computing and robotics applications, which is developed by D-Robotics, Inc. It is equipped with the latest Sunrise 5 SoC, which is capable of handling complex scenarios such as machine vision, SLAM (Simultaneous Localisation and Map Building) and deep learning.\nSystem Setup Since the RDK X5 development kit does not have eMMC on board, the system image needs to be burned into a TF card. The steps are as follows:\nGo to the official download page and download the system image.\nUse a formatting utility (e.g. Rufus) to install the system into a TF card (at least 8GB).\nAfter installing, insert the SD card into the board and turn on the power (make sure the power output is greater than 5V3A). After the first power up, the graphical interface Xfce of the official RDK X5 image will be displayed.\nAfter startup, you can connect to Wi-Fi network via GUI or command line tool, or log in to the board via SSH or serial port for control.The baud rate of the serial port of RDK X5 is lower than that of X3, and the USB to TTL module (CH340) is integrated on the board, which makes it more convenient to use the serial port for debugging.\nImage Dataset Create We use Roboflow to create dataset, due to its conventient labeling toolset. You can create your own project in Projects, assign the different photos in the dataset to different classes, and train the data using the default data distribution ratio. You can first classify a small part of the data, and then train a simpler model, and then use the model to automatically classify other still unclassified data more quickly and easily, and I only need to check and modify the classified data.\nTraining your YOLO Model Training your own YOLO (You Only Look Once) model involves several key steps, including preparing your dataset, setting up the environment, configuring the model, training, and evaluating the model. Below is a detailed guide:\n1. Setup the Environment First, ensure you have the necessary software and tools installed. Popular choices include:\nPython: Make sure you have Python 3.7 or higher installed.\nPyTorch: Install PyTorch for the neural network operations. You can install it via pip:\n1 pip install torch torchvision YOLO Framework: Choose a YOLO framework. Common choices are:\nDarknet: The original YOLO framework by Joseph Redmon. Ultralytics YOLOv5: A popular PyTorch implementation with ease of use. YOLOv8 by Ultralytics: Latest version, simpler setup. Install YOLOv5 using:\n1 pip install ultralytics For other versions like YOLOv3 using Darknet, you may need to clone the GitHub repository directly.\n2. Prepare the Dataset You need a labeled dataset for training:\nCollect Images: Gather images for each class you want to detect.\nAnnotate Images: Use a tool like LabelImg, Roboflow, or VGG Image Annotator to create annotations. Save annotations in YOLO format (or convert them to the correct format).\nYOLO annotation format example (saved as a .txt file for each image):\n1 \u0026lt;object-class\u0026gt; \u0026lt;x_center\u0026gt; \u0026lt;y_center\u0026gt; \u0026lt;width\u0026gt; \u0026lt;height\u0026gt; \u0026lt;object-class\u0026gt;: Integer representing the class label (e.g., 0, 1, 2\u0026hellip;). \u0026lt;x_center\u0026gt;, \u0026lt;y_center\u0026gt;: Normalized coordinates (relative to image width and height) of the bounding box center. \u0026lt;width\u0026gt;, \u0026lt;height\u0026gt;: Normalized width and height of the bounding box. Folder Structure:\n1 2 3 4 5 6 7 8 9 /dataset /images /train /val /test /labels /train /val /test Data File: Create a YAML or .data file specifying the dataset paths and classes. Example (for YOLOv5):\n1 2 3 4 5 6 train: dataset/images/train val: dataset/images/val test: dataset/images/test nc: 3 # number of classes names: [\u0026#39;class1\u0026#39;, \u0026#39;class2\u0026#39;, \u0026#39;class3\u0026#39;] 3. Configure the Model If you are using a pre-trained YOLO model as a base (recommended):\nDownload a Pre-trained Model: Start with a pre-trained model (e.g., yolov5s.pt for YOLOv5). Modify Configuration: Update the configuration to match your dataset. This usually involves modifying the number of classes. 4. Training the Model Run the training script using your dataset. Examples:\nYOLOv5 Training 1 yolo detect train data=data.yaml model=yolov5s.pt epochs=100 imgsz=640 data=data.yaml: Path to your data configuration file. model=yolov5s.pt: Pre-trained model (can be any YOLOv5 model or your custom model). epochs=100: Number of training epochs. imgsz=640: Image size for training. Darknet YOLOv4 Example Edit the yolov4.cfg file to set the number of classes and filters:\nChange classes to the number of your classes. Modify the filters parameter in the [convolutional] layer before each [yolo] layer: 1 filters = (classes + 5) * 3 Run the training:\n1 ./darknet detector train data/obj.data cfg/yolov4-custom.cfg yolov4.conv.137 5. Evaluating the Model After training, test the model to evaluate its performance:\n1 yolo detect val model=runs/train/exp/weights/best.pt data=data.yaml You can visualize predictions and compute metrics like Precision, Recall, mAP (mean Average Precision). 7. Inference \u0026amp; Deployment Use the trained model to perform inference on new images. Deploy the model on a web server, mobile device, or edge device. Example for inference using YOLOv5:\n1 yolo detect predict model=runs/train/exp/weights/best.pt source=your_image.jpg Deploy your model to RDK X5 Export to ONNX Modify the output header to make sure it is a 4-dimensional NHWC output Modify. /models/yolo.py file, Detect class, forward method, about 22 lines. Note: It is recommended that you keep the original forward method, e.g. change it to something else, such as forward_, to make it easier to switch back during training.\n1 2 def forward(self, x). return [self.m[i](x[i]).permute(0,2,3,1).contiguous() for i in range(self.nl)] Place . /models/export.py file to the root directory\n1 cp . /models/export.py export.py Make the following changes in line 14 of the export.py file to change the input pt file path and onnx resolution.\n1 2 3 parser.add_argument(\u0026#39;--weights\u0026#39;, type=str, default=\u0026#39;./yolov5s_tag2.0.pt\u0026#39;, help=\u0026#39;weights path\u0026#39;) parser.add_argument(\u0026#39;--img-size\u0026#39;, nargs=\u0026#39;+\u0026#39;, type=int, default=[640, 640], help=\u0026#39;image size\u0026#39;) parser.add_argument(\u0026#39;--batch-size\u0026#39;, type=int, default=1, help=\u0026#39;batch size\u0026#39;) Commented out 32 lines, 60 lines of code exported to other formats, and modified the following code blocks. Mainly opset version, also added an onnx simplify program, do some graph optimization, constant folding operation.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 try: import onnx from onnxsim import simplify print(\u0026#39;\\nStarting ONNX export with onnx %s...\u0026#39; % onnx.__version__) f = opt.weights.replace(\u0026#39;.pt\u0026#39;, \u0026#39;.onnx\u0026#39;) # filename model.fuse() # only for ONNX torch.onnx.export(model, img, f, verbose=False, opset_version=11, input_names=[\u0026#39;images\u0026#39;], output_names=[\u0026#39;small\u0026#39;, \u0026#39;medium\u0026#39;, \u0026#39;big\u0026#39;]) # Checks onnx_model = onnx.load(f) # load onnx model onnx.checker.check_model(onnx_model) # check onnx model print(onnx.helper.printable_graph(onnx_model.graph)) # print a human readable model # simplify onnx_model, check = simplify( onnx_model, dynamic_input_shape=False, input_shapes=None) assert check, \u0026#39;assert check failed\u0026#39; onnx.save(onnx_model, f) print(\u0026#39;ONNX export success, saved as %s\u0026#39; % f) except Exception as e: print(\u0026#39;ONNX export failure: %s\u0026#39; % e) Execute the export.py, export our own model to onnx that fits RDK X5\nEnvironment setup Your can get the convert toolset from official website. Here is the command I execute\n1 2 3 wget -c ftp://x5ftp@vrftp.horizon.ai/OpenExplorer/v1.2.8_release/docker_openexplorer_ubuntu_20_x5_cpu_v1.2.8.tar.gz --ftp-password=x5ftp@123$% wget -c ftp://x5ftp@vrftp.horizon.ai/OpenExplorer/v1.2.8_release/horizon_x5_open_explorer_v1.2.8-py310_20240926.tar.gz --ftp-password=x5ftp@123$% wget -c ftp://x5ftp@vrftp.horizon.ai/OpenExplorer/v1.2.8_release/x5_doc-v1.2.8-py310-en.zip --ftp-password=x5ftp@123$% After fetch the docker and tool, you need to load the docker image\n1 sudo docker load --input docker_openexplorer_ubuntu_20_x5_cpu_v1.2.8.tar.gz After you load image successfully, use the command below to start container\n1 docker run -it -v `pwd`:.open_explorer openexplorer/ai_toolchain_ubuntu_20_x5_cpu ","date":"2024-11-17T13:39:31Z","permalink":"https://zainz004.github.io/p/rdkx5/","title":"Rdkx5"},{"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\nBlockquote with attribution Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n— Rob Pike1\nTables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\nName Age Bob 27 Alice 23 Inline Markdown within tables Italics Bold Code italics bold code A B C D E F Lorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus ultricies, sapien non euismod aliquam, dui ligula tincidunt odio, at accumsan nulla sapien eget ex. Proin eleifend dictum ipsum, non euismod ipsum pulvinar et. Vivamus sollicitudin, quam in pulvinar aliquam, metus elit pretium purus Proin sit amet velit nec enim imperdiet vehicula. Ut bibendum vestibulum quam, eu egestas turpis gravida nec Sed scelerisque nec turpis vel viverra. Vivamus vitae pretium sapien Code Blocks Code block with backticks 1 2 3 4 5 6 7 8 9 10 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block indented with four spaces \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Diff code block 1 2 3 4 5 [dependencies.bevy] git = \u0026#34;https://github.com/bevyengine/bevy\u0026#34; rev = \u0026#34;11f52b8c72fc3a568e8bb4a4cd1f3eb025ac2e13\u0026#34; - features = [\u0026#34;dynamic\u0026#34;] + features = [\u0026#34;jpeg\u0026#34;, \u0026#34;dynamic\u0026#34;] One line code block 1 \u0026lt;p\u0026gt;A paragraph\u0026lt;/p\u0026gt; List Types Ordered List First item Second item Third item Unordered List List item Another item And another item Nested list Fruit Apple Orange Banana Dairy Milk Cheese Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL + ALT + Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\nThe above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2023-09-07T00:00:00Z","permalink":"https://zainz004.github.io/p/markdown-syntax-guide/","title":"Markdown Syntax Guide"},{"content":"Hugo theme Stack supports the creation of interactive image galleries using Markdown. It\u0026rsquo;s powered by PhotoSwipe and its syntax was inspired by Typlog.\nTo use this feature, the image must be in the same directory as the Markdown file, as it uses Hugo\u0026rsquo;s page bundle feature to read the dimensions of the image. External images are not supported.\nSyntax 1 ![Image 1](1.jpg) ![Image 2](2.jpg) Result Photo by mymind and Luke Chesser on Unsplash\n","date":"2023-08-26T00:00:00Z","image":"https://zainz004.github.io/p/image-gallery/2_hu15576070775610481867.jpg","permalink":"https://zainz004.github.io/p/image-gallery/","title":"Image gallery"},{"content":"For more details, check out the documentation.\nBilibili video Tencent video YouTube video Generic video file Your browser doesn't support HTML5 video. Here is a link to the video instead. Gist GitLab Quote Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n― A famous person, The book they wrote Photo by Codioful on Unsplash\n","date":"2023-08-25T00:00:00Z","image":"https://zainz004.github.io/p/shortcodes/cover_hu17063188895770243625.jpg","permalink":"https://zainz004.github.io/p/shortcodes/","title":"Shortcodes"},{"content":"Stack has built-in support for math typesetting using KaTeX.\nIt\u0026rsquo;s not enabled by default side-wide, but you can enable it for individual posts by adding math: true to the front matter. Or you can enable it side-wide by adding math = true to the params.article section in config.toml.\nInline math This is an inline mathematical expression: $\\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887…$\n1 $\\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887…$ Block math $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$ 1 2 3 $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$ $$ f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)\\,e^{2 \\pi i \\xi x}\\,d\\xi $$ 1 2 3 $$ f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)\\,e^{2 \\pi i \\xi x}\\,d\\xi $$ ","date":"2023-08-24T00:00:00Z","permalink":"https://zainz004.github.io/p/math-typesetting/","title":"Math Typesetting"}]