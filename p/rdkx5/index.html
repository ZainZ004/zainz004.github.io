<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="Intro RDK X5 is a high-performance robotics development kit designed for intelligent computing and robotics applications, which is developed by D-Robotics, Inc. It is equipped with the latest Sunrise 5 SoC, which is capable of handling complex scenarios such as machine vision, SLAM (Simultaneous Localisation and Map Building) and deep learning.\nSystem Setup Since the RDK X5 development kit does not have eMMC on board, the system image needs to be burned into a TF card. The steps are as follows:\n"><title>Rdkx5</title>
<link rel=canonical href=https://zainz004.github.io/p/rdkx5/><link rel=stylesheet href=/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css><meta property='og:title' content="Rdkx5"><meta property='og:description' content="Intro RDK X5 is a high-performance robotics development kit designed for intelligent computing and robotics applications, which is developed by D-Robotics, Inc. It is equipped with the latest Sunrise 5 SoC, which is capable of handling complex scenarios such as machine vision, SLAM (Simultaneous Localisation and Map Building) and deep learning.\nSystem Setup Since the RDK X5 development kit does not have eMMC on board, the system image needs to be burned into a TF card. The steps are as follows:\n"><meta property='og:url' content='https://zainz004.github.io/p/rdkx5/'><meta property='og:site_name' content="Zain's Hugo Test"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:published_time' content='2024-11-17T13:39:31+00:00'><meta property='article:modified_time' content='2024-11-17T13:39:31+00:00'><meta name=twitter:title content="Rdkx5"><meta name=twitter:description content="Intro RDK X5 is a high-performance robotics development kit designed for intelligent computing and robotics applications, which is developed by D-Robotics, Inc. It is equipped with the latest Sunrise 5 SoC, which is capable of handling complex scenarios such as machine vision, SLAM (Simultaneous Localisation and Map Building) and deep learning.\nSystem Setup Since the RDK X5 development kit does not have eMMC on board, the system image needs to be burned into a TF card. The steps are as follows:\n"><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu897059592634026878.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>üç•</span></figure><div class=site-meta><h1 class=site-name><a href=/>Zain's Hugo Test</a></h1><h2 class=site-description>Lorem ipsum dolor sit amet, consectetur adipiscing elit.</h2></div></header><ol class=menu-social><li><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://twitter.com target=_blank title=Twitter rel=me><svg class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Links</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#intro>Intro</a></li><li><a href=#system-setup>System Setup</a></li><li><a href=#image-dataset-create>Image Dataset Create</a></li><li><a href=#training-your-yolo-model>Training your YOLO Model</a><ol><li><a href=#1-setup-the-environment>1. <strong>Setup the Environment</strong></a></li><li><a href=#2-prepare-the-dataset>2. <strong>Prepare the Dataset</strong></a></li><li><a href=#3-configure-the-model>3. <strong>Configure the Model</strong></a></li><li><a href=#4-training-the-model>4. <strong>Training the Model</strong></a><ol><li><a href=#yolov5-training><strong>YOLOv5 Training</strong></a></li><li><a href=#darknet-yolov4-example><strong>Darknet YOLOv4 Example</strong></a></li></ol></li><li><a href=#5-evaluating-the-model>5. <strong>Evaluating the Model</strong></a></li><li><a href=#7-inference--deployment>7. <strong>Inference & Deployment</strong></a></li></ol></li><li><a href=#deploy-your-model-to-rdk-x5>Deploy your model to RDK X5</a><ol><li><a href=#export-to-onnx>Export to ONNX</a></li><li><a href=#environment-setup>Environment setup</a></li></ol></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><div class=article-title-wrapper><h2 class=article-title><a href=/p/rdkx5/>Rdkx5</a></h2></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Nov 17, 2024</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>6 minute read</time></div></footer></div></header><section class=article-content><h2 id=intro>Intro</h2><p>RDK X5 is a high-performance robotics development kit designed for intelligent computing and robotics applications, which is developed by D-Robotics, Inc. It is equipped with the latest Sunrise 5 SoC, which is capable of handling complex scenarios such as machine vision, SLAM (Simultaneous Localisation and Map Building) and deep learning.</p><h2 id=system-setup>System Setup</h2><p>Since the RDK X5 development kit does not have eMMC on board, the system image needs to be burned into a TF card. The steps are as follows:</p><ol><li><p>Go to the <a class=link href=https://archive.d-robotics.cc/downloads/os_images/rdk_x5/rdk_os_3.0.1-2024-10-18/ target=_blank rel=noopener>official download page</a> and download the system image.</p></li><li><p>Use a formatting utility (e.g. Rufus) to install the system into a TF card (at least 8GB).</p></li><li><p>After installing, insert the SD card into the board and turn on the power (make sure the power output is greater than 5V3A). After the first power up, the graphical interface Xfce of the official RDK X5 image will be displayed.</p></li></ol><p>After startup, you can connect to Wi-Fi network via GUI or command line tool, or log in to the board via SSH or serial port for control.The baud rate of the serial port of RDK X5 is lower than that of X3, and the USB to TTL module (CH340) is integrated on the board, which makes it more convenient to use the serial port for debugging.</p><h2 id=image-dataset-create>Image Dataset Create</h2><p>We use <a class=link href=https://roboflow.com/ target=_blank rel=noopener>Roboflow</a> to create dataset, due to its conventient labeling toolset.
<img src=/pics/roboflow.png loading=lazy>
You can create your own project in Projects, assign the different photos in the dataset to different classes, and train the data using the default data distribution ratio. You can first classify a small part of the data, and then train a simpler model, and then use the model to automatically classify other still unclassified data more quickly and easily, and I only need to check and modify the classified data.</p><h2 id=training-your-yolo-model>Training your YOLO Model</h2><p>Training your own YOLO (You Only Look Once) model involves several key steps, including preparing your dataset, setting up the environment, configuring the model, training, and evaluating the model. Below is a detailed guide:</p><h3 id=1-setup-the-environment>1. <strong>Setup the Environment</strong></h3><p>First, ensure you have the necessary software and tools installed. Popular choices include:</p><ul><li><p><strong>Python</strong>: Make sure you have Python 3.7 or higher installed.</p></li><li><p><strong>PyTorch</strong>: Install PyTorch for the neural network operations. You can install it via pip:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip install torch torchvision
</span></span></code></pre></td></tr></table></div></div></li><li><p><strong>YOLO Framework</strong>: Choose a YOLO framework. Common choices are:</p><ul><li><strong>Darknet</strong>: The original YOLO framework by Joseph Redmon.</li><li><strong>Ultralytics YOLOv5</strong>: A popular PyTorch implementation with ease of use.</li><li><strong>YOLOv8</strong> by Ultralytics: Latest version, simpler setup.</li></ul></li></ul><p>Install YOLOv5 using:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip install ultralytics
</span></span></code></pre></td></tr></table></div></div><p>For other versions like YOLOv3 using Darknet, you may need to clone the GitHub repository directly.</p><h3 id=2-prepare-the-dataset>2. <strong>Prepare the Dataset</strong></h3><p>You need a labeled dataset for training:</p><ol><li><p><strong>Collect Images</strong>: Gather images for each class you want to detect.</p></li><li><p><strong>Annotate Images</strong>: Use a tool like <strong>LabelImg</strong>, <strong>Roboflow</strong>, or <strong>VGG Image Annotator</strong> to create annotations. Save annotations in YOLO format (or convert them to the correct format).</p><p>YOLO annotation format example (saved as a <code>.txt</code> file for each image):</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>&lt;object-class&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;
</span></span></code></pre></td></tr></table></div></div><ul><li><strong><code>&lt;object-class></code></strong>: Integer representing the class label (e.g., 0, 1, 2&mldr;).</li><li><strong><code>&lt;x_center>, &lt;y_center></code></strong>: Normalized coordinates (relative to image width and height) of the bounding box center.</li><li><strong><code>&lt;width>, &lt;height></code></strong>: Normalized width and height of the bounding box.</li></ul></li><li><p><strong>Folder Structure</strong>:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>/dataset
</span></span><span class=line><span class=cl>    /images
</span></span><span class=line><span class=cl>        /train
</span></span><span class=line><span class=cl>        /val
</span></span><span class=line><span class=cl>        /test
</span></span><span class=line><span class=cl>    /labels
</span></span><span class=line><span class=cl>        /train
</span></span><span class=line><span class=cl>        /val
</span></span><span class=line><span class=cl>        /test
</span></span></code></pre></td></tr></table></div></div></li><li><p><strong>Data File</strong>: Create a YAML or <code>.data</code> file specifying the dataset paths and classes. Example (for YOLOv5):</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>train</span><span class=p>:</span><span class=w> </span><span class=l>dataset/images/train</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>val</span><span class=p>:</span><span class=w> </span><span class=l>dataset/images/val</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>test</span><span class=p>:</span><span class=w> </span><span class=l>dataset/images/test</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>nc</span><span class=p>:</span><span class=w> </span><span class=m>3</span><span class=w>  </span><span class=c># number of classes</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>names</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=s1>&#39;class1&#39;</span><span class=p>,</span><span class=w> </span><span class=s1>&#39;class2&#39;</span><span class=p>,</span><span class=w> </span><span class=s1>&#39;class3&#39;</span><span class=p>]</span><span class=w>
</span></span></span></code></pre></td></tr></table></div></div></li></ol><h3 id=3-configure-the-model>3. <strong>Configure the Model</strong></h3><p>If you are using a pre-trained YOLO model as a base (recommended):</p><ol><li><strong>Download a Pre-trained Model</strong>: Start with a pre-trained model (e.g., <code>yolov5s.pt</code> for YOLOv5).</li><li><strong>Modify Configuration</strong>: Update the configuration to match your dataset. This usually involves modifying the number of classes.</li></ol><h3 id=4-training-the-model>4. <strong>Training the Model</strong></h3><p>Run the training script using your dataset. Examples:</p><h4 id=yolov5-training><strong>YOLOv5 Training</strong></h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>yolo detect train <span class=nv>data</span><span class=o>=</span>data.yaml <span class=nv>model</span><span class=o>=</span>yolov5s.pt <span class=nv>epochs</span><span class=o>=</span><span class=m>100</span> <span class=nv>imgsz</span><span class=o>=</span><span class=m>640</span>
</span></span></code></pre></td></tr></table></div></div><ul><li><strong><code>data=data.yaml</code></strong>: Path to your data configuration file.</li><li><strong><code>model=yolov5s.pt</code></strong>: Pre-trained model (can be any YOLOv5 model or your custom model).</li><li><strong><code>epochs=100</code></strong>: Number of training epochs.</li><li><strong><code>imgsz=640</code></strong>: Image size for training.</li></ul><h4 id=darknet-yolov4-example><strong>Darknet YOLOv4 Example</strong></h4><p>Edit the <code>yolov4.cfg</code> file to set the number of classes and filters:</p><ul><li>Change <code>classes</code> to the number of your classes.</li><li>Modify the <code>filters</code> parameter in the <code>[convolutional]</code> layer before each <code>[yolo]</code> layer:<div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>filters = (classes + 5) * 3
</span></span></code></pre></td></tr></table></div></div></li></ul><p>Run the training:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>./darknet detector train data/obj.data cfg/yolov4-custom.cfg yolov4.conv.137
</span></span></code></pre></td></tr></table></div></div><h3 id=5-evaluating-the-model>5. <strong>Evaluating the Model</strong></h3><p>After training, test the model to evaluate its performance:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>yolo detect val <span class=nv>model</span><span class=o>=</span>runs/train/exp/weights/best.pt <span class=nv>data</span><span class=o>=</span>data.yaml
</span></span></code></pre></td></tr></table></div></div><ul><li>You can visualize predictions and compute metrics like <strong>Precision, Recall, mAP (mean Average Precision)</strong>.</li></ul><h3 id=7-inference--deployment>7. <strong>Inference & Deployment</strong></h3><ul><li>Use the trained model to perform inference on new images.</li><li>Deploy the model on a web server, mobile device, or edge device.</li></ul><p>Example for inference using YOLOv5:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>yolo detect predict <span class=nv>model</span><span class=o>=</span>runs/train/exp/weights/best.pt <span class=nv>source</span><span class=o>=</span>your_image.jpg
</span></span></code></pre></td></tr></table></div></div><h2 id=deploy-your-model-to-rdk-x5>Deploy your model to RDK X5</h2><h3 id=export-to-onnx>Export to ONNX</h3><p>Modify the output header to make sure it is a 4-dimensional NHWC output Modify. /models/yolo.py file, Detect class, forward method, about 22 lines. Note: It is recommended that you keep the original forward method, e.g. change it to something else, such as forward_, to make it easier to switch back during training.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>def forward(self, x).
</span></span><span class=line><span class=cl>    return [self.m[i](x[i]).permute(0,2,3,1).contiguous() for i in range(self.nl)]
</span></span></code></pre></td></tr></table></div></div><p>Place <code>. /models/export.py</code> file to the root directory</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-gdscript3 data-lang=gdscript3><span class=line><span class=cl><span class=n>cp</span> <span class=o>.</span> <span class=o>/</span><span class=n>models</span><span class=o>/</span><span class=k>export</span><span class=o>.</span><span class=n>py</span> <span class=k>export</span><span class=o>.</span><span class=n>py</span>
</span></span></code></pre></td></tr></table></div></div><p>Make the following changes in line 14 of the export.py file to change the input pt file path and onnx resolution.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>parser.add_argument(&#39;--weights&#39;, type=str, default=&#39;./yolov5s_tag2.0.pt&#39;, help=&#39;weights path&#39;)
</span></span><span class=line><span class=cl>parser.add_argument(&#39;--img-size&#39;, nargs=&#39;+&#39;, type=int, default=[640, 640], help=&#39;image size&#39;)
</span></span><span class=line><span class=cl>parser.add_argument(&#39;--batch-size&#39;, type=int, default=1, help=&#39;batch size&#39;)
</span></span></code></pre></td></tr></table></div></div><p>Commented out 32 lines, 60 lines of code exported to other formats, and modified the following code blocks. Mainly opset version, also added an onnx simplify program, do some graph optimization, constant folding operation.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-gdscript3 data-lang=gdscript3><span class=line><span class=cl><span class=n>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>import</span> <span class=n>onnx</span>
</span></span><span class=line><span class=cl>    <span class=n>from</span> <span class=n>onnxsim</span> <span class=n>import</span> <span class=n>simplify</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;</span><span class=se>\n</span><span class=s1>Starting ONNX export with onnx </span><span class=si>%s</span><span class=s1>...&#39;</span> <span class=o>%</span> <span class=n>onnx</span><span class=o>.</span><span class=n>__version__</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>f</span> <span class=o>=</span> <span class=n>opt</span><span class=o>.</span><span class=n>weights</span><span class=o>.</span><span class=n>replace</span><span class=p>(</span><span class=s1>&#39;.pt&#39;</span><span class=p>,</span> <span class=s1>&#39;.onnx&#39;</span><span class=p>)</span>  <span class=c1># filename</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>fuse</span><span class=p>()</span>  <span class=c1># only for ONNX</span>
</span></span><span class=line><span class=cl>    <span class=n>torch</span><span class=o>.</span><span class=n>onnx</span><span class=o>.</span><span class=k>export</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>img</span><span class=p>,</span> <span class=n>f</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=n>False</span><span class=p>,</span> <span class=n>opset_version</span><span class=o>=</span><span class=mi>11</span><span class=p>,</span> <span class=n>input_names</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;images&#39;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                      <span class=n>output_names</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;small&#39;</span><span class=p>,</span> <span class=s1>&#39;medium&#39;</span><span class=p>,</span> <span class=s1>&#39;big&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=c1># Checks</span>
</span></span><span class=line><span class=cl>    <span class=n>onnx_model</span> <span class=o>=</span> <span class=n>onnx</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>f</span><span class=p>)</span>  <span class=c1># load onnx model</span>
</span></span><span class=line><span class=cl>    <span class=n>onnx</span><span class=o>.</span><span class=n>checker</span><span class=o>.</span><span class=n>check_model</span><span class=p>(</span><span class=n>onnx_model</span><span class=p>)</span>  <span class=c1># check onnx model</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>onnx</span><span class=o>.</span><span class=n>helper</span><span class=o>.</span><span class=n>printable_graph</span><span class=p>(</span><span class=n>onnx_model</span><span class=o>.</span><span class=n>graph</span><span class=p>))</span>  <span class=c1># print a human readable model</span>
</span></span><span class=line><span class=cl>    <span class=c1># simplify</span>
</span></span><span class=line><span class=cl>    <span class=n>onnx_model</span><span class=p>,</span> <span class=n>check</span> <span class=o>=</span> <span class=n>simplify</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>onnx_model</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>dynamic_input_shape</span><span class=o>=</span><span class=n>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>input_shapes</span><span class=o>=</span><span class=n>None</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>assert</span> <span class=n>check</span><span class=p>,</span> <span class=s1>&#39;assert check failed&#39;</span>
</span></span><span class=line><span class=cl>    <span class=n>onnx</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=n>onnx_model</span><span class=p>,</span> <span class=n>f</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;ONNX export success, saved as </span><span class=si>%s</span><span class=s1>&#39;</span> <span class=o>%</span> <span class=n>f</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>except</span> <span class=n>Exception</span> <span class=n>as</span> <span class=n>e</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;ONNX export failure: </span><span class=si>%s</span><span class=s1>&#39;</span> <span class=o>%</span> <span class=n>e</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Execute the <code>export.py</code>, export our own model to onnx that fits RDK X5</p><h3 id=environment-setup>Environment setup</h3><p>Your can get the convert toolset from official website. Here is the command I execute</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>wget -c ftp://x5ftp@vrftp.horizon.ai/OpenExplorer/v1.2.8_release/docker_openexplorer_ubuntu_20_x5_cpu_v1.2.8.tar.gz --ftp-password=x5ftp@123$%
</span></span><span class=line><span class=cl>wget -c ftp://x5ftp@vrftp.horizon.ai/OpenExplorer/v1.2.8_release/horizon_x5_open_explorer_v1.2.8-py310_20240926.tar.gz --ftp-password=x5ftp@123$%
</span></span><span class=line><span class=cl>wget -c ftp://x5ftp@vrftp.horizon.ai/OpenExplorer/v1.2.8_release/x5_doc-v1.2.8-py310-en.zip --ftp-password=x5ftp@123$%
</span></span></code></pre></td></tr></table></div></div><p>After fetch the docker and tool, you need to load the docker image</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-gdscript3 data-lang=gdscript3><span class=line><span class=cl><span class=n>sudo</span> <span class=n>docker</span> <span class=nb>load</span> <span class=o>--</span><span class=n>input</span> <span class=n>docker_openexplorer_ubuntu_20_x5_cpu_v1</span><span class=o>.</span><span class=mf>2.8</span><span class=o>.</span><span class=n>tar</span><span class=o>.</span><span class=n>gz</span>
</span></span></code></pre></td></tr></table></div></div><p>After you load image successfully, use the command below to start container</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-mysql data-lang=mysql><span class=line><span class=cl><span class=n>docker</span><span class=w> </span><span class=n>run</span><span class=w> </span><span class=o>-</span><span class=n>it</span><span class=w> </span><span class=o>-</span><span class=n>v</span><span class=w> </span><span class=o>`</span><span class=n>pwd</span><span class=o>`</span><span class=p>:.</span><span class=n>open_explorer</span><span class=w> </span><span class=n>openexplorer</span><span class=o>/</span><span class=n>ai_toolchain_ubuntu_20_x5_cpu</span><span class=w>
</span></span></span></code></pre></td></tr></table></div></div></section><footer class=article-footer><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer></article><footer class=site-footer><section class=copyright>&copy;
2020 -
2024 Zain's Hugo Test</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.29.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>